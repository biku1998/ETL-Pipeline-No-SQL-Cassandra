# Imports
import numpy as np
import pandas as pd 
import cassandra
import re
import glob
import os
import json
import csv
from cassandra.cluster import Cluster
from tqdm import tqdm

from SQL_Queries import *


def process_csv():
	""" 
	This method will create ETL pipeline to process the csv files under event_data/
	"""
	# check current working directory
	print("Current working directory : ",os.getcwd())

	filepath = os.getcwd()+"/event_data"

	print(filepath)

	for root,dirs,files in os.walk(filepath):

		filepath_list = glob.glob(os.path.join(root,'*'))
		# print(filepath_list)

	# let's create an empty list that will store the data row generated by each file

	full_data_list = []

	for f in filepath_list:

		# read that csv file
		with open(f,'r',encoding='utf8',newline='') as csv_file:

			# creating a csv-reader instance
			csvreader = csv.reader(csv_file)
			next(csvreader)

			# extracting each row data one-by-one and appending it to list
			for line in csvreader:
				# print(line) # for debug
				full_data_list.append(line)
	print("#"*55)
	print("total data rows :",len(full_data_list))
	print("Sample data list :",full_data_list[0]) # it is list of lists
	print("#"*55)



	# we will now create a smaller dataset that we will insert into the cassandra database.
	csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)

	with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:
	    writer = csv.writer(f, dialect='myDialect')
	    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\
	                'level','location','sessionId','song','userId'])
	    for row in full_data_list:
	        if (row[0] == ''):
	            continue
	        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))

	        # check the number of rows in your csv file
	with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:
	    print("no of records in the new datafile : ",sum(1 for line in f))

def insert_data():

	try:
		cluster  = Cluster()
		session = cluster.connect()
		session.set_keyspace("sparkifydb")
		""" 
		This method will insert data into Apache Cassandra database from event_datafile_new.csv
		"""
		file = 'event_datafile_new.csv'

		print("Inserting data in song_details","#"*30)

		with open(file,encoding='utf8') as f:
			csvreader = csv.reader(f)
			next(csvreader) # skip the header
			for line in tqdm(csvreader,total=6820):
				session.execute(INSERT_SONG_DETAILS,(int(line[8]), int(line[3]), line[0], line[9], float(line[5])))

		print("Inserting data in artist_details","#"*30)

		with open(file,encoding='utf8') as f:
			csvreader = csv.reader(f)
			next(csvreader) # skip the header

			for line in tqdm(csvreader,total=6820):
				session.execute(INSERT_ARTIST_DETAILS,(int(line[10]), int(line[8]), int(line[3]), line[0], line[9], line[1], line[4]))		

		print("Inserting data in user_details","#"*30)

		with open(file,encoding='utf8') as f:
			csvreader = csv.reader(f)
			next(csvreader) # skip the header

			for line in tqdm(csvreader,total=6820):
				session.execute(INSERT_USER_DETAILS,(line[9], int(line[10]), line[1], line[4]))		
	except Exception as e:
		print(e)


def create_database():
	""" 
	This method will create the keyspace i.e database and the tables
	"""
	try:
		cluster = Cluster()

		session  = cluster.connect()

		session.execute(CREATE_KEYSPACE)

		session.set_keyspace('sparkifydb')

		session.execute(CREATE_TABLE_SONG_DETAILS)

		session.execute(CREATE_TABLE_ARTIST_DETAILS)

		session.execute(CREATE_TABLE_USER_DETAILS)

	except Exception as e:
		print(e)

def drop_database():
	try:
		cluster = Cluster()

		session = cluster.connect()

		session.execute(DROP_SONG_DETAILS)

		session.execute(DROP_ARTIST_DETAILS)

		session.execute(DROP_USER_DETAILS)

		session.execute(DROP_KEYSPACE)

	except Exception as e:
		print(e)

def perform_select_query():
	""" This method will perform all the select queries on all the tables in the keyspace"""
	try:
		cluster = Cluster()
		session = cluster.connect()
		session.set_keyspace("sparkifydb")
		print(" Results from song_details : ","#"*30)

		rows = session.execute(SONG_DETAIL_SELECT, (338, 4))
		for row in rows:
	   		print("Artist: "+row.artist, "\nSong Title: "+row.song_title, "\nSong Length: "+str(row.song_length))
	
		print(" Results from Album_details : ","#"*30)

		rows = session.execute(ARTIST_DETAIL_SELECT,(10, 182))
		for row in rows:
	   		print("Artist: "+row.artist_name, "\nSong Title: "+row.song_title, "\nUser First Name: "+row.user_first_name, "\nUser Last Name: "+row.user_last_name)

		print(" Results from Users_details : ","#"*30)

		rows = session.execute(USER_DETAIL_SELECT,('All Hands Against His Own', ))
		for row in rows:
			print("User First Name: "+row.user_first_name, "\nUser Last Name: "+row.user_last_name)	
		session.shutdown()
		cluster.shutdown()
	except Exception as e:	
		print(e)


def main():
	drop_database()
	create_database()
	process_csv()
	insert_data()
	perform_select_query()


if __name__ == "__main__":
	main()



